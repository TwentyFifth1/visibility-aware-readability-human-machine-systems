# Visibility-Aware Readability (Human–Machine Systems)

This repository contains reproducible, publication-grade materials for studying text readability feasibility under controlled, classroom-like viewing configurations. The work is situated in the domain of human–machine systems, with a deliberate emphasis on perceptual accessibility rather than user intent, cognition, or downstream behavioral outcomes.

This repository is created as an **individual submission** for the Human–Machine Systems course (Week 17: Implementation & GitHub Deployment). The scientific design, implementation, and experimental results follow the group project repository referenced below.

## Project Motivation and Goal

The primary goal of this project is to model whether text is readable *in principle* (`can_read`) given a specific viewing setup. These setups include variations in viewing distance, angular size, head pose, display medium, and contrast conditions.

Rather than focusing on subjective preference or comprehension outcomes, the study investigates perceptual feasibility: whether the physical and perceptual conditions permit readability at all. A key research question is whether a human-rated visibility score alone is predictively sufficient for readability when compared against richer geometric and environmental feature sets.

## Reference Repository (Group Project)

The original group project repository, which contains the full implementation and experimental artifacts, is available at:

https://github.com/NattakittiP/visibility_aware_readability

All core scientific contributions, code design, and reported results originate from the group project repository above. This repository exists to satisfy the individual GitHub submission requirement.

## Scientific Scope and Interpretation

The dataset includes a human-rated visibility score that serves as a perceptual proxy for text readability. Because this score is derived from human judgment, it is inherently correlated with the binary readability label (`can_read`).

As a result, all analyses presented in the referenced work should be interpreted carefully:

- The findings reflect **predictive sufficiency** within the measured regime.
- The results do not imply causal sufficiency.
- Deployability in unconstrained or real-world classroom environments is not claimed.
- The contribution is methodological, emphasizing careful validation, uncertainty quantification, and interpretability.

## Repository Organization (Original Design)

The referenced GitHub repository is intentionally structured to support clean reproducibility and transparent auditing. It is split into two main branches:

### Code
This branch contains all executable source code used in the study. It includes data preprocessing, model training, evaluation pipelines, and analysis scripts. All scripts are designed with leakage-safe preprocessing and consistent experimental protocols.

### Result
This branch contains all artifacts generated by the code. These include CSV result tables, figures, and LaTeX files that correspond exactly to the reported experimental results. This separation between code and results ensures reviewer-friendly inspection and reproducibility.

## Experimental Components Reproduced

Running the code from the referenced repository reproduces all components reported in the associated manuscript, including:

### 1. Scenario-Level Evaluation
- Repeated stratified cross-validation
- Pooled out-of-fold (OOF) predictions
- Condition-blocked cross-validation as a proxy for deployment shift

### 2. Interpretable Modeling
- L2-regularized Logistic Regression
- Random Forest classifiers
- Consistent feature preprocessing pipelines

### 3. Probability Calibration
- Uncalibrated baseline models
- Platt scaling
- Isotonic regression  
(All calibration methods are fit strictly on training-only predictions.)

### 4. Explainability and Interpretation
- Impurity-based feature importance
- Permutation importance
- One- and two-dimensional Partial Dependence Plots (PDPs)

### 5. Sufficiency-Oriented Analyses (Predictive, Not Causal)
- Feature ablation studies:
  - Geometry-only features
  - Visibility-only features
  - Geometry plus visibility features
- Paired AUC comparisons using pooled OOF predictions
- Residualization-style stress tests
- Information-theoretic summaries as implemented

### 6. Robustness Diagnostics
- Structured perturbations including:
  - Visibility bias and clipping
  - Head pose quantization
  - Feature noise injection
- Performance degradation tracking under perturbations

### 7. Uncertainty-Aware Evaluation
- Bootstrap confidence intervals
- Subgroup and worst-case performance reporting
- Split conformal prediction under scenario-level splits
- Mondrian (group-conditional) conformal coverage

## Dataset Assumptions

All scripts expect a CSV file named:

### Required Columns

**Target**
- `can_read` (binary)

**Visibility**
- `visibility_score`

**Geometry**
- `distance_m`
- `font_size_pt`
- `text_height_mm`
- `angular_size_deg`

**Head Pose**
- `head_yaw_deg`
- `head_pitch_deg`
- `head_roll_deg`

**Viewing Conditions**
- `medium`
- `contrast`

**Optional**
- `participant_id`

If `participant_id` is absent, some scripts generate balanced pseudo-participants strictly for stress-testing and leave-one-participant-out (LOPO)-style evaluation. These are not treated as real subjects.

## Reproducibility Practices

The referenced implementation emphasizes reproducibility and methodological rigor:

- Fixed random seeds (`RANDOM_STATE = 42`) throughout
- Leakage-safe preprocessing pipelines
- Deterministic outputs except where stochastic repetition is intentional
- Clear separation of code and results via GitHub branches

## Final Note

This repository is maintained to fulfill the **individual GitHub repository requirement** for the course. For full implementation details, experimental pipelines, and reproducible results, please refer to the group project repository linked above.

